/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
/******/ var __webpack_modules__ = ({

/***/ "./src/scripts/audio/AudioProcessor.js":
/*!*********************************************!*\
  !*** ./src/scripts/audio/AudioProcessor.js ***!
  \*********************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   AudioProcessor: () => (/* binding */ AudioProcessor)\n/* harmony export */ });\n/* harmony import */ var _InitializedState_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./InitializedState.js */ \"./src/scripts/audio/InitializedState.js\");\n\nclass AudioProcessor {\n  constructor(options = {}) {\n    // limit to one instance for whole scope\n    if (AudioProcessor.instance) {\n      return AudioProcessor.instance;\n    }\n    this.audioContext = new (window.AudioContext || window.webkitAudioContext)();\n    this.analyser = this.audioContext.createAnalyser();\n    this.analyser.fftSize = 2048;\n    this.analyser.smoothingTimeConstant = 0.8;\n    this.bufferLength = this.analyser.frequencyBinCount;\n    this.dataArray = new Uint8Array(this.analyser.frequencyBinCount);\n    this.state = new _InitializedState_js__WEBPACK_IMPORTED_MODULE_0__.InitializedState(this.audioContext, this.analyser);\n    this.isActive = false;\n    this.isPlaying = true;\n    this.source = null;\n    this.startTime = null;\n    this.debugMode = options.debugMode || false;\n    AudioProcessor.instance = this;\n  }\n  static getInstance() {\n    if (!AudioProcessor.instance) {\n      AudioProcessor.instance = new AudioProcessor();\n    }\n    return AudioProcessor.instance;\n  }\n  async play() {\n    this.isPlaying = true;\n    this.audioContext.resume();\n  }\n  async pause() {\n    this.isPlaying = false;\n    this.audioContext.suspend();\n  }\n  async initMicrophone() {\n    if (!this.isActive) {\n      this.state = await this.state.initMicrophone();\n      this.isActive = true;\n      this.audioContext.resume();\n      if (this.debugMode) {\n        console.log('initMicrophone');\n      }\n    } else {\n      if (this.debugMode) {\n        console.log('already active');\n      }\n    }\n  }\n  async initFile(file) {\n    if (!this.isActive) {\n      this.state = await this.state.initFile(file);\n      this.isActive = true;\n      this.audioContext.resume();\n      if (this.debugMode) {\n        console.log('initFile:', file.name);\n      }\n    } else {\n      if (this.debugMode) {\n        console.log('already active');\n      }\n    }\n  }\n  initMockData() {\n    if (!this.isActive) {\n      this.state = this.state.initMockData();\n      this.isActive = true;\n      this.audioContext.resume();\n      if (this.debugMode) {\n        console.log('initMockData');\n      }\n    } else {\n      if (this.debugMode) {\n        console.log('already active');\n      }\n    }\n  }\n  stop() {\n    if (this.isActive) {\n      this.state = this.state.stop();\n      this.isActive = false;\n      if (this.debugMode) {\n        console.log('Audio processing stopped');\n      }\n    } else {\n      if (this.debugMode) {\n        console.log('already inactive');\n      }\n    }\n  }\n\n  /**\r\n   * Set debug mode for the audio processor\r\n   * Controls logging, analyzer settings, and processing behavior\r\n   * \r\n   * @param {boolean} enabled - Whether debug mode should be enabled\r\n   */\n  setDebugMode(enabled) {\n    // Store the previous value to detect changes\n    const previousMode = this.debugMode;\n\n    // Update the flag\n    this.debugMode = enabled;\n\n    // If this is a mode change, log it\n    if (previousMode !== enabled) {\n      console.log(`Debug mode ${enabled ? 'enabled' : 'disabled'}`);\n\n      // Adjust analyzer settings for debug mode\n      if (enabled) {\n        // In debug mode, use more precise settings for better diagnostic data\n        this.analyser.smoothingTimeConstant = 0.5; // Less smoothing for more responsive debugging\n\n        // Log current state\n        console.log('Audio processor state:', {\n          isActive: this.isActive,\n          isPlaying: this.isPlaying,\n          fftSize: this.analyser.fftSize,\n          smoothingTimeConstant: this.analyser.smoothingTimeConstant,\n          bufferLength: this.bufferLength\n        });\n      } else {\n        // In production mode, optimize for performance\n        this.analyser.smoothingTimeConstant = 0.8; // More smoothing for better visual quality\n      }\n    }\n\n    // If we have an active connection, logging information about it\n    if (this.isActive && enabled) {\n      console.log('Active audio connection with current state:', this.state);\n    }\n  }\n\n  // TODO: Check if active for below functions\n  getFrequencyData() {\n    this._normalizeFrequencyData(this.state.getFrequencyData(this.dataArray));\n    return this.dataArray;\n  }\n  getTimeDomainData() {\n    this._normalizeTimeDomainData(this.state.getTimeDomainData(this.dataArray));\n    return this.dataArray;\n  }\n  getFrequencyDataForAPI() {\n    this._normalizeFrequencyData(this.getFrequencyData(), this.dataArray);\n    return this.dataArray.slice();\n  }\n  getTimeDomainDataForAPI() {\n    this._normalizeTimeDomainData(this.getTimeDomainData(), this.dataArray);\n    return this.dataArray.slice();\n  }\n  getFrequencyBinCount() {\n    return this.analyser.frequencyBinCount;\n  }\n\n  /**\r\n   * Helper function to normalize frequency data from dB (-100 to 0) to 0-255 range\r\n   * @param {Float32Array} sourceData - Raw frequency data (typically -100 to 0 dB)\r\n   * @param {Uint8Array} targetArray - Target array for normalized values\r\n   * @private\r\n   */\n  _normalizeFrequencyData(sourceData, targetArray) {\n    for (let i = 0; i < sourceData.length; i++) {\n      // Frequency data is typically in -100 to 0 dB range\n      // Map to 0-255 range, where -100dB = 0 and 0dB = 255\n      const normalizedValue = (sourceData[i] + 100) / 100;\n      targetArray[i] = Math.floor(Math.max(0, Math.min(255, normalizedValue * 255)));\n    }\n  }\n\n  /**\r\n   * Helper function to normalize time domain data from -1,1 to 0-255 range\r\n   * @param {Float32Array} sourceData - Raw time domain data (-1 to 1)\r\n   * @param {Uint8Array} targetArray - Target array for normalized values\r\n   * @private\r\n   */\n  _normalizeTimeDomainData(sourceData, targetArray) {\n    for (let i = 0; i < sourceData.length; i++) {\n      // Time domain data is in -1 to 1 range\n      // Map -1.0 to 1.0 to 0-255 range\n      const normalizedValue = (sourceData[i] + 1) / 2;\n      targetArray[i] = Math.floor(Math.max(0, Math.min(255, normalizedValue * 255)));\n    }\n  }\n}\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/audio/AudioProcessor.js?");

/***/ }),

/***/ "./src/scripts/audio/GPUAudioProcessor.js":
/*!************************************************!*\
  !*** ./src/scripts/audio/GPUAudioProcessor.js ***!
  \************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   GPUAudioProcessor: () => (/* binding */ GPUAudioProcessor)\n/* harmony export */ });\n/* harmony import */ var _AudioProcessor_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./AudioProcessor.js */ \"./src/scripts/audio/AudioProcessor.js\");\n\n/**\r\n * GPUAudioProcessor - Audio analysis class with WebGL/THREE.js integration\r\n * Handles audio input sources and converts audio data to GPU-friendly formats\r\n */\nclass GPUAudioProcessor extends _AudioProcessor_js__WEBPACK_IMPORTED_MODULE_0__.AudioProcessor {\n  /**\r\n   * Create a new GPUAudioProcessor\r\n   * @param {Object} options - Configuration options\r\n   * @param {number} [options.fftSize=2048] - FFT size for frequency analysis\r\n   * @param {number} [options.smoothingTimeConstant=0.8] - Smoothing factor for analysis\r\n   * @param {boolean} [options.debugMode=false] - Enable console debugging output\r\n   */\n  constructor(options = {}) {\n    super(); // Call parent constructor to initialize base state\n\n    // Get the audio context and analyzer from the parent class\n    this.audioCtx = this.audioContext;\n    this.analyser = this.analyser;\n\n    // Update analyzer with provided options\n    if (options.fftSize) this.analyser.fftSize = options.fftSize;\n    if (options.smoothingTimeConstant) this.analyser.smoothingTimeConstant = options.smoothingTimeConstant;\n\n    // Create data buffers for GPU-specific formats (Float32 vs Uint8 in base class)\n    this.frequencyBinCount = this.analyser.frequencyBinCount;\n    this.frequencyData = new Float32Array(this.frequencyBinCount);\n    this.timeDomainData = new Float32Array(this.frequencyBinCount);\n\n    // For debugging\n    this.debugMode = options.debugMode || false;\n\n    // Create data textures for WebGL\n    this.frequencyDataTexture = this.createDataTexture(this.frequencyData);\n    this.timeDomainDataTexture = this.createDataTexture(this.timeDomainData);\n  }\n\n  /**\r\n   * Creates a THREE.js texture from audio data\r\n   * @param {Float32Array} data - Audio data to convert to texture\r\n   * @returns {THREE.DataTexture} Texture containing the audio data\r\n   */\n  createDataTexture(data) {\n    const texture = new THREE.DataTexture(data, data.length, 1, THREE.RedFormat, THREE.FloatType);\n    texture.needsUpdate = true;\n    return texture;\n  }\n\n  /**\r\n   * Updates the internal textures with fresh audio data\r\n   * Called each frame to get new audio analysis\r\n   * Side-effects: Updates frequencyData, timeDomainData, and their textures\r\n   */\n  updateTextureData() {\n    // Get fresh data\n    this.analyser.getFloatFrequencyData(this.frequencyData);\n    this.analyser.getFloatTimeDomainData(this.timeDomainData);\n    this.frequencyDataTexture.needsUpdate = true;\n    this.timeDomainDataTexture.needsUpdate = true;\n  }\n\n  /**\r\n   * Initializes microphone input\r\n   * Overrides base class method to add GPU-specific handling\r\n   * @returns {Promise<void>} Promise that resolves when mic is initialized\r\n   */\n  async initMicrophone() {\n    // Call the base class implementation to handle state transitions\n    await super.initMicrophone();\n\n    // Get fresh data for our GPU textures\n    this.updateTextureData();\n    if (this.debugMode) {\n      console.log('GPU-enhanced microphone initialized');\n    }\n  }\n\n  /**\r\n   * Initializes audio from a file\r\n   * Overrides base class method to add GPU-specific handling\r\n   * @param {File} file - The audio file to play\r\n   * @returns {Promise<void>} Promise that resolves when file is initialized\r\n   */\n  async initFile(file) {\n    // Call the base class implementation to handle state transitions\n    await super.initFile(file);\n\n    // Get fresh data for our GPU textures\n    this.updateTextureData();\n    if (this.debugMode) {\n      console.log('GPU-enhanced file playback initialized');\n    }\n  }\n\n  /**\r\n   * Initializes audio from a URL (for demo or preset audio)\r\n   * New GPU-specific method not in base class\r\n   * @param {string} url - The URL of the audio file to play\r\n   * @returns {Promise<void>} Promise that resolves when audio starts playing\r\n   */\n  async initFromUrl(url) {\n    // Fetch and decode the audio data\n    try {\n      const response = await fetch(url);\n      if (!response.ok) {\n        throw new Error(`HTTP error! Status: ${response.status}`);\n      }\n      const arrayBuffer = await response.arrayBuffer();\n      const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);\n\n      // Create and set up a source using equivalent of initFile\n      // This simulates a file being loaded but from a URL\n      const file = new File([arrayBuffer], \"audio-from-url.mp3\", {\n        type: \"audio/mpeg\"\n      });\n      await this.initFile(file);\n      if (this.debugMode) {\n        console.log('Playing audio from URL:', url);\n      }\n    } catch (err) {\n      console.error('Error loading audio from URL:', err);\n    }\n  }\n\n  /**\r\n   * Stops any currently playing audio source\r\n   * Overrides base class method to add GPU-specific cleanup\r\n   */\n  stop() {\n    // Call the base class implementation to handle state transitions\n    super.stop();\n\n    // Clear our GPU textures or reset them to default values\n    this.frequencyData.fill(0);\n    this.timeDomainData.fill(0);\n    this.frequencyDataTexture.needsUpdate = true;\n    this.timeDomainDataTexture.needsUpdate = true;\n    if (this.debugMode) {\n      console.log('GPU audio processing stopped');\n    }\n  }\n\n  /**\r\n   * Pauses the current audio source\r\n   * Overrides base class method\r\n   */\n  async pause() {\n    // Call the base class implementation\n    await super.pause();\n    if (this.debugMode) {\n      console.log('GPU audio processing paused');\n    }\n  }\n\n  /**\r\n   * Resumes the current audio source\r\n   * Overrides base class method\r\n   */\n  async play() {\n    // Call the base class implementation\n    await super.play();\n    if (this.debugMode) {\n      console.log('GPU audio processing resumed');\n    }\n  }\n\n  /**\r\n   * @deprecated Use stop() instead\r\n   */\n  stopCurrentSource() {\n    console.warn('stopCurrentSource() is deprecated, use stop() instead');\n    this.stop();\n  }\n\n  /**\r\n   * @deprecated Use pause() instead\r\n   */\n  pauseCurrentSource() {\n    console.warn('pauseCurrentSource() is deprecated, use pause() instead');\n    this.pause();\n  }\n\n  /**\r\n   * @deprecated Use play() instead\r\n   */\n  resumeCurrentSource() {\n    console.warn('resumeCurrentSource() is deprecated, use play() instead');\n    this.play();\n  }\n\n  /**\r\n   * Returns raw frequency domain data\r\n   * @returns {Float32Array} Frequency data in decibels (typically -100 to 0 dB)\r\n   */\n  getFrequencyData() {\n    return this.frequencyData;\n  }\n\n  /**\r\n   * Returns raw time domain data\r\n   * @returns {Float32Array} Time domain data (values between -1 and 1)\r\n   */\n  getTimeDomainData() {\n    // Make sure we have the latest data\n    this.analyser.getFloatTimeDomainData(this.timeDomainData);\n    return this.timeDomainData;\n  }\n\n  /**\r\n   * Returns the frequency data texture for use in shaders\r\n   * @returns {THREE.DataTexture} Texture containing frequency data\r\n   */\n  getFrequencyDataTexture() {\n    return this.frequencyDataTexture;\n  }\n\n  /**\r\n   * Returns the time domain data texture for use in shaders\r\n   * @returns {THREE.DataTexture} Texture containing time domain data\r\n   */\n  getTimeDomainDataTexture() {\n    return this.timeDomainDataTexture;\n  }\n\n  /**\r\n   * Cleans up all resources used by this processor\r\n   * Should be called when the processor is no longer needed\r\n   */\n  dispose() {\n    // First stop any active audio\n    this.stop();\n\n    // Clean up WebGL resources\n    if (this.frequencyDataTexture) {\n      this.frequencyDataTexture.dispose();\n      this.frequencyDataTexture = null;\n    }\n    if (this.timeDomainDataTexture) {\n      this.timeDomainDataTexture.dispose();\n      this.timeDomainDataTexture = null;\n    }\n\n    // The base class manages the AudioContext cleanup\n    // so we don't need to close it manually\n\n    if (this.debugMode) {\n      console.log('GPU audio processor resources disposed');\n    }\n  }\n\n  /**\r\n   * Connects an external analyzer node instead of the internal one\r\n   * @param {AnalyserNode} analyser - Web Audio analyzer node to use\r\n   */\n  connectExternalAnalyser(analyser) {\n    this.analyser = analyser;\n    console.log('Connected external analyzer');\n  }\n\n  /**\r\n   * Sets the volume of the audio output\r\n   * @param {number} volume - Volume level between 0 and 1\r\n   */\n  setVolume(volume) {\n    // Get the current state\n    const currentState = this.state;\n\n    // Use a more moderate scaling curve - square root for more audible range at low volumes\n    // This makes lower volume settings more usable while still providing good control\n    const scaledVolume = Math.sqrt(volume);\n\n    // Store the current volume value for reference\n    this.volume = volume;\n\n    // Call the state's setVolume method if available\n    if (currentState && typeof currentState.setVolume === 'function') {\n      // The state will handle all volume implementation\n      // Pass the original volume, not the scaled volume, to avoid double-scaling\n      currentState.setVolume(volume);\n\n      // Log for debugging\n      console.log('Volume set via state method:', volume, 'Scaled volume (not used):', scaledVolume);\n\n      // We've delegated volume control to the state, so no need to also do it here\n      // This prevents double-application of volume changes\n      return;\n    }\n\n    // Fallback: If state doesn't have a setVolume method, handle it here\n    if (currentState && currentState.gainNode) {\n      // If the state has a gain node, use it\n      currentState.gainNode.gain.value = volume === 0 ? 0 : Math.max(0.0001, scaledVolume);\n      console.log('Volume set directly on state gain node:', volume, 'Applied gain:', currentState.gainNode.gain.value);\n    } else if (this.gainNode) {\n      // Use our local gain node as fallback\n      this.gainNode.gain.value = volume === 0 ? 0 : Math.max(0.0001, scaledVolume);\n      console.log('Volume set on processor gain node:', volume, 'Applied gain:', this.gainNode.gain.value);\n    } else {\n      // Create a gain node if none exists\n      this.gainNode = this.audioContext.createGain();\n      this.gainNode.gain.value = volume === 0 ? 0 : Math.max(0.0001, scaledVolume);\n\n      // Connect through the state system if possible\n      if (this.analyser) {\n        try {\n          this.analyser.disconnect();\n          this.analyser.connect(this.gainNode);\n          this.gainNode.connect(this.audioContext.destination);\n          console.log('Created new gain node and set up connections');\n        } catch (e) {\n          console.error('Error setting up gain node:', e);\n        }\n      }\n      console.log('Created new gain node with volume:', volume, 'Applied gain:', this.gainNode.gain.value);\n    }\n    if (this.debugMode) {\n      console.log('Volume set to:', volume, 'Scaled volume:', scaledVolume);\n    }\n  }\n\n  /**\r\n   * Updates the internal analyzer settings and recreates data buffers\r\n   * Should be called after changing FFT size or other analyzer settings\r\n   */\n  updateAnalyzerSettings() {\n    // Get the current fft size and smoothing\n    const fftSize = this.analyser.fftSize;\n    const smoothingTimeConstant = this.analyser.smoothingTimeConstant;\n\n    // Store current values to restore after setup\n    this.fftSize = fftSize;\n    this.smoothingTimeConstant = smoothingTimeConstant;\n\n    // Update our data buffers after analyzer settings change\n    this.frequencyBinCount = this.analyser.frequencyBinCount;\n    this.frequencyData = new Float32Array(this.frequencyBinCount);\n    this.timeDomainData = new Float32Array(this.frequencyBinCount);\n\n    // Recreate textures with new size\n    if (this.frequencyDataTexture) {\n      this.frequencyDataTexture.dispose();\n    }\n    if (this.timeDomainDataTexture) {\n      this.timeDomainDataTexture.dispose();\n    }\n    this.frequencyDataTexture = this.createDataTexture(this.frequencyData);\n    this.timeDomainDataTexture = this.createDataTexture(this.timeDomainData);\n    if (this.debugMode) {\n      console.log(`Analyzer settings updated: fftSize=${fftSize}, smoothing=${smoothingTimeConstant}`);\n    }\n  }\n\n  /**\r\n   * Set the FFT size for frequency analysis\r\n   * @param {number} size - Must be a power of 2 between 32 and 32768\r\n   */\n  setFFTSize(size) {\n    // Ensure size is a valid FFT size (power of 2)\n    const validSizes = [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768];\n    if (!validSizes.includes(size)) {\n      console.error('FFT size must be a power of 2 between 32 and 32768');\n      return;\n    }\n\n    // Set the size on the analyzer\n    this.analyser.fftSize = size;\n\n    // Update our buffers and textures\n    this.updateAnalyzerSettings();\n  }\n\n  /**\r\n   * @deprecated Use getFrequencyDataTexture() and getTimeDomainDataTexture() instead\r\n   */\n  reconnectAnalyzer(connectToOutput = true) {\n    console.warn('reconnectAnalyzer() is deprecated as it may interfere with state management');\n    // Legacy implementation maintained for backward compatibility\n    try {\n      this.analyser.disconnect();\n      if (connectToOutput) {\n        if (this.gainNode) {\n          this.analyser.connect(this.gainNode);\n          this.gainNode.connect(this.audioContext.destination);\n        } else {\n          this.analyser.connect(this.audioContext.destination);\n        }\n      }\n    } catch (e) {\n      console.error('Error in reconnectAnalyzer:', e);\n    }\n  }\n\n  /**\r\n   * @deprecated Use the state pattern methods instead\r\n   */\n  setupAnalyser() {\n    console.warn('setupAnalyser() is deprecated as it may interfere with state management');\n    this.updateAnalyzerSettings();\n  }\n\n  /**\r\n   * @deprecated Use updateAnalyzerSettings() instead\r\n   */\n  ensureAudioContext() {\n    console.warn('ensureAudioContext() is deprecated. Use updateAnalyzerSettings() instead');\n    // Forward to play() which handles resuming the AudioContext\n    this.play();\n  }\n\n  /**\r\n   * Override the base class getFrequencyDataForAPI method to use our Float32 data\r\n   * @returns {Uint8Array} Normalized frequency data in 0-255 range\r\n   */\n  getFrequencyDataForAPI() {\n    // Update our Float32 data first\n    this.analyser.getFloatFrequencyData(this.frequencyData);\n\n    // Then convert to Uint8Array format for API compatibility\n    const uint8Array = new Uint8Array(this.frequencyBinCount);\n    this._normalizeFrequencyData(this.frequencyData, uint8Array);\n    if (this.debugMode && uint8Array.length > 0) {\n      console.log(\"Converted frequency data sample:\", Array.from(uint8Array.slice(0, 5)));\n    }\n    return uint8Array;\n  }\n\n  /**\r\n   * Override the base class getTimeDomainDataForAPI method to use our Float32 data\r\n   * @returns {Uint8Array} Normalized time domain data in 0-255 range\r\n   */\n  getTimeDomainDataForAPI() {\n    // Update our Float32 data first\n    this.analyser.getFloatTimeDomainData(this.timeDomainData);\n\n    // Then convert to Uint8Array format for API compatibility\n    const uint8Array = new Uint8Array(this.frequencyBinCount);\n    this._normalizeTimeDomainData(this.timeDomainData, uint8Array);\n    return uint8Array;\n  }\n\n  /**\r\n   * Helper function to normalize frequency data from dB (-100 to 0) to 0-255 range\r\n   * @param {Float32Array} sourceData - Raw frequency data (typically -100 to 0 dB)\r\n   * @param {Uint8Array} targetArray - Target array for normalized values\r\n   * @private\r\n   */\n  _normalizeFrequencyData(sourceData, targetArray) {\n    for (let i = 0; i < this.frequencyBinCount; i++) {\n      // Frequency data is typically in -100 to 0 dB range\n      // Map to 0-255 range, where -100dB = 0 and 0dB = 255\n      const normalizedValue = (sourceData[i] + 100) / 100;\n      targetArray[i] = Math.floor(Math.max(0, Math.min(255, normalizedValue * 255)));\n    }\n  }\n\n  /**\r\n   * Helper function to normalize time domain data from -1,1 to 0-255 range\r\n   * @param {Float32Array} sourceData - Raw time domain data (-1 to 1)\r\n   * @param {Uint8Array} targetArray - Target array for normalized values\r\n   * @private\r\n   */\n  _normalizeTimeDomainData(sourceData, targetArray) {\n    for (let i = 0; i < this.frequencyBinCount; i++) {\n      // Time domain data is in -1 to 1 range\n      // Map -1.0 to 1.0 to 0-255 range\n      const normalizedValue = (sourceData[i] + 1) / 2;\n      targetArray[i] = Math.floor(Math.max(0, Math.min(255, normalizedValue * 255)));\n    }\n  }\n\n  /**\r\n   * Initializes mock data for testing\r\n   * Overrides base class method\r\n   */\n  initMockData() {\n    // Call the base class implementation\n    super.initMockData();\n\n    // Update our GPU textures\n    this.updateTextureData();\n    if (this.debugMode) {\n      console.log('GPU-enhanced mock data initialized');\n    }\n  }\n}\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/audio/GPUAudioProcessor.js?");

/***/ }),

/***/ "./src/scripts/audio/InitializedState.js":
/*!***********************************************!*\
  !*** ./src/scripts/audio/InitializedState.js ***!
  \***********************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   InitializedState: () => (/* binding */ InitializedState)\n/* harmony export */ });\n// Remove the circular imports at the top\n// import { FileState } from \"./FileState.js\";\n// import { MicrophoneState } from \"./MicrophoneState.js\";\n// import { NetworkStreamState } from \"./NetworkStreamState.js\";\n\n/**\r\n * Base audio state that handles common functionality and state transitions.\r\n * All specific audio state implementations should extend this class.\r\n */\nclass InitializedState {\n  constructor(audioContext, analyser) {\n    this.audioContext = audioContext;\n    this.analyser = analyser;\n    this.source = null;\n\n    // Create a gain node by default for all audio states\n    this.gainNode = this.audioContext.createGain();\n    // Default to full volume\n    this.gainNode.gain.value = 1.0;\n\n    // Connect analyzer to gain node by default\n    // Child classes should use this.gainNode in their setup\n    try {\n      this.analyser.connect(this.gainNode);\n      this.gainNode.connect(this.audioContext.destination);\n    } catch (e) {\n      console.error('Error setting up default gain node:', e);\n    }\n  }\n\n  /**\r\n   * Sets the volume for this audio state\r\n   * @param {number} volume - Volume level between 0 and 1\r\n   */\n  setVolume(volume) {\n    if (!this.gainNode) {\n      this.gainNode = this.audioContext.createGain();\n      try {\n        // Reconnect if needed\n        this.analyser.connect(this.gainNode);\n        this.gainNode.connect(this.audioContext.destination);\n      } catch (e) {\n        console.error('Error setting up gain node in setVolume:', e);\n      }\n    }\n\n    // Use square root scaling for more natural volume control\n    // This gives more audible range at low volume settings, while still allowing for fine control\n    const scaledVolume = Math.sqrt(volume);\n\n    // Set the gain value - ensure 0 is actually 0, otherwise use scaled value\n    this.gainNode.gain.value = volume === 0 ? 0 : scaledVolume;\n    console.log('Base state volume set to:', volume, 'Scaled volume:', scaledVolume);\n  }\n\n  /**\r\n   * Initializes microphone input\r\n   * @returns {Promise<MicrophoneState>} Promise that resolves to new state\r\n   */\n  async initMicrophone() {\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({\n        audio: true\n      });\n      const source = this.audioContext.createMediaStreamSource(stream);\n      source.connect(this.analyser);\n\n      // Use dynamic import to avoid circular dependency\n      const {\n        MicrophoneState\n      } = await __webpack_require__.e(/*! import() */ \"src_scripts_audio_MicrophoneState_js\").then(__webpack_require__.bind(__webpack_require__, /*! ./MicrophoneState.js */ \"./src/scripts/audio/MicrophoneState.js\"));\n      return new MicrophoneState(this.audioContext, this.analyser, source, stream);\n    } catch (error) {\n      console.error(\"Error accessing microphone:\", error);\n      return this;\n    }\n  }\n\n  /**\r\n   * Initializes audio from a file\r\n   * @param {File} file - The audio file to play\r\n   * @returns {Promise<FileState>} Promise that resolves to new state\r\n   */\n  async initFile(file) {\n    try {\n      const arrayBuffer = await file.arrayBuffer();\n      const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);\n\n      // Use dynamic import to avoid circular dependency\n      const {\n        FileState\n      } = await __webpack_require__.e(/*! import() */ \"src_scripts_audio_FileState_js\").then(__webpack_require__.bind(__webpack_require__, /*! ./FileState.js */ \"./src/scripts/audio/FileState.js\"));\n      return new FileState(this.audioContext, this.analyser, audioBuffer);\n    } catch (error) {\n      console.error(\"Error loading audio file:\", error);\n      return this;\n    }\n  }\n\n  /**\r\n   * Initializes audio from a network stream\r\n   * @param {string} streamUrl - URL of the stream to play\r\n   * @returns {Promise<NetworkStreamState>} Promise that resolves to new state\r\n   */\n  async initNetworkStream(streamUrl) {\n    try {\n      // Use dynamic import to avoid circular dependency\n      const {\n        NetworkStreamState\n      } = await __webpack_require__.e(/*! import() */ \"src_scripts_audio_NetworkStreamState_js\").then(__webpack_require__.bind(__webpack_require__, /*! ./NetworkStreamState.js */ \"./src/scripts/audio/NetworkStreamState.js\"));\n      return new NetworkStreamState(this.audioContext, this.analyser, streamUrl);\n    } catch (error) {\n      console.error(\"Error initializing network stream:\", error);\n      return this;\n    }\n  }\n\n  /**\r\n   * Creates mock audio data for testing\r\n   * @returns {InitializedState} This state with mock data\r\n   */\n  initMockData() {\n    // Clean up current source if it exists\n    this.stop();\n    this.source = this.audioContext.createBufferSource();\n    const buffer = this.audioContext.createBuffer(1, this.audioContext.sampleRate * 3, this.audioContext.sampleRate);\n    const data = buffer.getChannelData(0);\n\n    // Generate mock data (e.g., sine wave or random data)\n    for (let i = 0; i < data.length; i++) {\n      data[i] = Math.random() * 2 - 1; // Random data between -1 and 1\n    }\n    this.source.buffer = buffer;\n    this.source.loop = true;\n    this.source.connect(this.analyser);\n    this.source.start();\n    console.log(\"Mock data initialized\");\n    return this;\n  }\n\n  /**\r\n   * Stops and cleans up the current audio source\r\n   */\n  stop() {\n    if (this.source) {\n      this.source.disconnect();\n      if (this.source.stop) {\n        this.source.stop(0);\n      }\n      this.source = null;\n      console.log(\"Audio source stopped\");\n    }\n  }\n\n  /**\r\n   * Gets frequency data from the analyzer\r\n   * @param {Uint8Array} dataArray - Array to store frequency data\r\n   */\n  getFrequencyData(dataArray) {\n    this.analyser.getByteFrequencyData(dataArray);\n  }\n\n  /**\r\n   * Gets time domain data from the analyzer\r\n   * @param {Uint8Array} dataArray - Array to store time domain data\r\n   */\n  getTimeDomainData(dataArray) {\n    this.analyser.getByteTimeDomainData(dataArray);\n  }\n}\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/audio/InitializedState.js?");

/***/ }),

/***/ "./src/scripts/visualizers/circular-waveform.js":
/*!******************************************************!*\
  !*** ./src/scripts/visualizers/circular-waveform.js ***!
  \******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   registerCircularWaveform: () => (/* binding */ registerCircularWaveform)\n/* harmony export */ });\n// Circular Waveform Visualizer\n// Creates a circular waveform that responds to audio amplitude\n\n/**\r\n * Register the circular waveform visualizer component\r\n */\nfunction registerCircularWaveform() {\n  if (AFRAME.components['circular-waveform']) {\n    console.log('circular-waveform already registered');\n    return;\n  }\n  AFRAME.registerComponent('circular-waveform', {\n    schema: {\n      analyserNode: {\n        type: 'selector'\n      },\n      radius: {\n        type: 'number',\n        default: 5\n      },\n      height: {\n        type: 'number',\n        default: 2\n      },\n      color: {\n        type: 'color',\n        default: '#FFF'\n      },\n      segments: {\n        type: 'int',\n        default: 128\n      }\n    },\n    init: function () {\n      console.log('Circular waveform initialized');\n      this.analyserNode = this.data.analyserNode;\n\n      // Create geometry\n      this.createGeometry();\n\n      // Listen for audio data updates\n      this.analyserNode.addEventListener('audiodata-updated', this.updateWaveform.bind(this));\n    },\n    createGeometry: function () {\n      // Create a circle of points\n      const segments = this.data.segments;\n      const radius = this.data.radius;\n\n      // Create geometry\n      const geometry = new THREE.BufferGeometry();\n\n      // Create positions array\n      const positions = new Float32Array(segments * 3);\n      const colors = new Float32Array(segments * 3);\n\n      // Initialize positions in a circle\n      for (let i = 0; i < segments; i++) {\n        const angle = i / segments * Math.PI * 2;\n        const x = Math.cos(angle) * radius;\n        const y = 0;\n        const z = Math.sin(angle) * radius;\n        positions[i * 3] = x;\n        positions[i * 3 + 1] = y;\n        positions[i * 3 + 2] = z;\n\n        // Rainbow colors\n        const hue = i / segments;\n        const color = new THREE.Color().setHSL(hue, 1, 0.5);\n        colors[i * 3] = color.r;\n        colors[i * 3 + 1] = color.g;\n        colors[i * 3 + 2] = color.b;\n      }\n      geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));\n      geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));\n\n      // Create material\n      const material = new THREE.LineBasicMaterial({\n        color: this.data.color,\n        vertexColors: true,\n        linewidth: 2\n      });\n\n      // Create line\n      this.line = new THREE.LineLoop(geometry, material);\n      this.el.setObject3D('mesh', this.line);\n\n      // Store reference to position attribute for updates\n      this.positions = geometry.attributes.position;\n    },\n    updateWaveform: function (event) {\n      if (!this.positions) return;\n      const timeDomainData = event.detail.timeDomainData;\n      const segments = this.data.segments;\n      const radius = this.data.radius;\n      const height = this.data.height;\n\n      // Update positions based on audio data\n      for (let i = 0; i < segments; i++) {\n        const dataIndex = Math.floor(i / segments * timeDomainData.length);\n        const value = timeDomainData[dataIndex] / 128.0 - 1.0; // Convert to -1 to 1 range\n\n        const angle = i / segments * Math.PI * 2;\n        const amplifiedValue = value * height;\n\n        // Calculate new position\n        const newRadius = radius + amplifiedValue;\n        const x = Math.cos(angle) * newRadius;\n        const y = amplifiedValue * 0.5; // Add some vertical movement\n        const z = Math.sin(angle) * newRadius;\n\n        // Update position\n        this.positions.array[i * 3] = x;\n        this.positions.array[i * 3 + 1] = y;\n        this.positions.array[i * 3 + 2] = z;\n      }\n\n      // Mark positions for update\n      this.positions.needsUpdate = true;\n    },\n    remove: function () {\n      if (this.analyserNode) {\n        this.analyserNode.removeEventListener('audiodata-updated', this.updateWaveform);\n      }\n      if (this.line) {\n        if (this.line.geometry) this.line.geometry.dispose();\n        if (this.line.material) this.line.material.dispose();\n      }\n      this.el.removeObject3D('mesh');\n    }\n  });\n}\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/visualizers/circular-waveform.js?");

/***/ }),

/***/ "./src/scripts/visualizers/frequency-bars.js":
/*!***************************************************!*\
  !*** ./src/scripts/visualizers/frequency-bars.js ***!
  \***************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   registerFrequencyBars: () => (/* binding */ registerFrequencyBars)\n/* harmony export */ });\n// Frequency Bars Visualizer\n// Creates a circular arrangement of bars that respond to audio frequency content\n\n/**\r\n * Register the frequency bars visualizer component\r\n */\nfunction registerFrequencyBars() {\n  if (AFRAME.components['frequency-bars']) {\n    console.log('frequency-bars already registered');\n    return;\n  }\n  AFRAME.registerComponent('frequency-bars', {\n    schema: {\n      analyserNode: {\n        type: 'selector'\n      },\n      radius: {\n        type: 'number',\n        default: 8\n      },\n      height: {\n        type: 'number',\n        default: 3\n      },\n      color: {\n        type: 'color',\n        default: '#FFF'\n      },\n      segments: {\n        type: 'int',\n        default: 64\n      },\n      smoothing: {\n        type: 'number',\n        default: 0.8\n      },\n      minDecibels: {\n        type: 'number',\n        default: -100\n      },\n      maxDecibels: {\n        type: 'number',\n        default: -30\n      }\n    },\n    init: function () {\n      console.log('Frequency bars initialized');\n      this.analyserNode = this.data.analyserNode;\n      this.averages = new Array(this.data.segments).fill(0);\n      this.frequencyRanges = this.calculateFrequencyRanges();\n\n      // Create geometry\n      this.createGeometry();\n\n      // Listen for audio data updates\n      this.analyserNode.addEventListener('audiodata-updated', this.updateBars.bind(this));\n    },\n    calculateFrequencyRanges: function () {\n      // Calculate logarithmic frequency ranges for better visualization\n      const ranges = [];\n      const minFreq = 20; // Hz\n      const maxFreq = 20000; // Hz\n      const segments = this.data.segments;\n      for (let i = 0; i < segments; i++) {\n        // Calculate frequency range for each bar using logarithmic scale\n        const freq = minFreq * Math.pow(maxFreq / minFreq, i / segments);\n        ranges.push(freq);\n      }\n      return ranges;\n    },\n    createGeometry: function () {\n      const segments = this.data.segments;\n      const radius = this.data.radius;\n\n      // Create a group to hold all the bars\n      this.barGroup = new THREE.Group();\n      this.el.setObject3D('mesh', this.barGroup);\n\n      // Create individual bars\n      this.bars = [];\n      for (let i = 0; i < segments; i++) {\n        const angle = i / segments * Math.PI * 2;\n        const x = Math.cos(angle) * radius;\n        const z = Math.sin(angle) * radius;\n\n        // Create bar geometry\n        const barWidth = 2 * Math.PI * radius / segments * 0.9; // 90% of available space\n        const barDepth = 0.5;\n        const geometry = new THREE.BoxGeometry(barWidth, 1, barDepth);\n\n        // Create material with gradient color based on frequency range\n        const freqRatio = this.frequencyRanges[i] / 20000; // Normalize to 0-1\n        const hue = 0.7 - freqRatio * 0.5; // Blue to red spectrum\n        const color = new THREE.Color().setHSL(hue, 1, 0.5);\n        const material = new THREE.MeshStandardMaterial({\n          color: color,\n          metalness: 0.3,\n          roughness: 0.7,\n          emissive: color,\n          emissiveIntensity: 0.3,\n          transparent: true,\n          opacity: 0.25\n        });\n\n        // Create mesh\n        const bar = new THREE.Mesh(geometry, material);\n\n        // Position the bar at the perimeter, on the ground\n        bar.position.set(x, 0, z);\n\n        // Rotate to face center\n        bar.lookAt(0, 0, 0);\n\n        // Set the pivot point to the bottom of the bar\n        bar.geometry.translate(0, 0.5, 0);\n\n        // Add to group\n        this.barGroup.add(bar);\n        this.bars.push(bar);\n      }\n    },\n    updateBars: function (event) {\n      if (!this.bars || !this.bars.length) return;\n\n      // Use frequency data for proper spectrum analysis\n      const frequencyData = event.detail.frequencyData;\n      const segments = this.data.segments;\n      const maxHeight = this.data.height;\n      const smoothing = this.data.smoothing;\n      if (!frequencyData || frequencyData.length === 0) return;\n\n      // Update each bar based on frequency data\n      for (let i = 0; i < segments; i++) {\n        // Calculate the frequency range for this bar\n        const freq = this.frequencyRanges[i];\n        const binIndex = Math.floor(freq / 22050 * frequencyData.length);\n\n        // Get normalized value (0-1) from frequency data\n        const value = (frequencyData[binIndex] - this.data.minDecibels) / (this.data.maxDecibels - this.data.minDecibels);\n\n        // Apply smoothing\n        this.averages[i] = this.averages[i] * smoothing + value * (1 - smoothing);\n\n        // Update bar height and material properties\n        const height = Math.max(0.01, this.averages[i] * maxHeight);\n        this.bars[i].scale.y = height;\n\n        // Update emissive intensity based on frequency power\n        const material = this.bars[i].material;\n        material.emissiveIntensity = 0.3 + this.averages[i] * 0.7;\n\n        // Add subtle color shift based on intensity\n        const baseHue = 0.7 - freq / 20000 * 0.5;\n        const intensityShift = this.averages[i] * 0.2; // Shift hue based on intensity\n        material.color.setHSL(baseHue + intensityShift, 1, 0.5);\n        material.emissive.setHSL(baseHue + intensityShift, 1, 0.5);\n      }\n    },\n    remove: function () {\n      if (this.analyserNode) {\n        this.analyserNode.removeEventListener('audiodata-updated', this.updateBars);\n      }\n      if (this.bars) {\n        this.bars.forEach(bar => {\n          if (bar.geometry) bar.geometry.dispose();\n          if (bar.material) bar.material.dispose();\n        });\n        this.bars = [];\n      }\n      if (this.barGroup) {\n        this.el.removeObject3D('mesh');\n      }\n    }\n  });\n}\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/visualizers/frequency-bars.js?");

/***/ }),

/***/ "./src/scripts/visualizers/index.js":
/*!******************************************!*\
  !*** ./src/scripts/visualizers/index.js ***!
  \******************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   registerVisualizers: () => (/* binding */ registerVisualizers)\n/* harmony export */ });\n/* harmony import */ var _wave3_script_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./wave3-script.js */ \"./src/scripts/visualizers/wave3-script.js\");\n/* harmony import */ var _circular_waveform_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./circular-waveform.js */ \"./src/scripts/visualizers/circular-waveform.js\");\n/* harmony import */ var _frequency_bars_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./frequency-bars.js */ \"./src/scripts/visualizers/frequency-bars.js\");\n// Main visualizer registration module\n\n\n\nfunction registerVisualizers() {\n  console.log('Registering visualizer components...');\n\n  // Register each visualizer\n  (0,_wave3_script_js__WEBPACK_IMPORTED_MODULE_0__.registerWave3Visualizer)();\n  (0,_circular_waveform_js__WEBPACK_IMPORTED_MODULE_1__.registerCircularWaveform)();\n  (0,_frequency_bars_js__WEBPACK_IMPORTED_MODULE_2__.registerFrequencyBars)();\n  console.log('All visualizer components registered');\n}\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/visualizers/index.js?");

/***/ }),

/***/ "./src/scripts/visualizers/wave3-script.js":
/*!*************************************************!*\
  !*** ./src/scripts/visualizers/wave3-script.js ***!
  \*************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   registerWave3Visualizer: () => (/* binding */ registerWave3Visualizer)\n/* harmony export */ });\n// This script creates a circular waveform visualization as a ring around the user\n// Using direct THREE.js lines instead of canvas textures for better performance\n\n// Export a function to register the component\n// This allows us to control when the component is registered\nfunction registerWave3Visualizer() {\n  console.log('Attempting to register wave3-visualizer component');\n\n  // Check if A-Frame is loaded\n  if (typeof AFRAME === 'undefined') {\n    console.error('A-Frame not loaded, cannot register wave3-visualizer');\n    return false;\n  }\n\n  // Skip if already registered\n  if (AFRAME.components['wave3-visualizer']) {\n    console.log('wave3-visualizer already registered');\n    return true;\n  }\n\n  // Register a component for the wave3 system\n  AFRAME.registerComponent('wave3-visualizer', {\n    /**\n     * Component schema definition\n     */\n    schema: {\n      audioProcessor: {\n        type: 'selector',\n        default: '#audio-processor'\n      },\n      width: {\n        type: 'number',\n        default: 4\n      },\n      // Width/length of the visualization lines\n      height: {\n        type: 'number',\n        default: 10\n      },\n      // Height of the visualization\n      radius: {\n        type: 'number',\n        default: 20\n      },\n      // Radius of the ring\n      segments: {\n        type: 'int',\n        default: 128\n      },\n      // Number of segments in the ring\n      lineWidth: {\n        type: 'number',\n        default: 3\n      },\n      // Line width for the waveform\n      colorStart: {\n        type: 'color',\n        default: '#ff3232'\n      },\n      // Start color\n      colorMiddle: {\n        type: 'color',\n        default: '#32ffff'\n      },\n      // Middle color\n      colorEnd: {\n        type: 'color',\n        default: '#3232ff'\n      },\n      // End color\n      responsiveness: {\n        type: 'number',\n        default: 1.0\n      },\n      // How responsive to audio (0.1-1.0)\n      breathingEffect: {\n        type: 'boolean',\n        default: true\n      },\n      // Enable breathing/pulsing effect\n      breathingSpeed: {\n        type: 'number',\n        default: 1.0\n      },\n      // Speed of breathing effect\n      breathingIntensity: {\n        type: 'number',\n        default: 0.1\n      },\n      // Intensity of breathing/pulsing\n      rotation: {\n        type: 'boolean',\n        default: true\n      },\n      // Enable rotation of the entire ring\n      rotationSpeed: {\n        type: 'number',\n        default: 1.0\n      },\n      // Rotation speed\n      frequencyInfluence: {\n        type: 'number',\n        default: 0.3\n      }\n    },\n    /**\n     * Initialize the component\n     */\n    init: function () {\n      console.log('wave3-visualizer init called');\n\n      // Initialize frequency analysis properties\n      this.frequencyBands = {\n        bass: {\n          min: 20,\n          max: 250\n        },\n        midrange: {\n          min: 250,\n          max: 2000\n        },\n        treble: {\n          min: 2000,\n          max: 20000\n        }\n      };\n      this.bandPowers = {\n        bass: 0,\n        midrange: 0,\n        treble: 0\n      };\n\n      // Create ring visualization with THREE.js lines\n      this.retryCount = 0;\n      this.maxRetries = 5;\n      this.createRingVisualization();\n\n      // Reference to our rendering group\n      this.group = this.el.getObject3D('mesh');\n\n      // Store previous data for smooth transitions\n      this.prevData = null;\n      this.smoothingFactor = 0.3; // Lower = smoother transitions\n\n      // Initialize animation variables\n      this.animationTime = 0;\n      this.breathingPhase = 0;\n      this.colorPhase = 0;\n\n      // Bind event handlers properly to maintain \"this\" context\n      this.boundUpdateVisualization = this.updateVisualization.bind(this);\n      this.boundAnimationLoop = this.animationLoop.bind(this);\n\n      // Set up event listeners\n      this.setupEventListeners();\n\n      // Force the first render to ensure visibility\n      this.forceRender();\n\n      // Set up animation loop if using dynamic effects\n      if (this.data.breathingEffect || this.data.rotation) {\n        this.animationFrame = requestAnimationFrame(this.boundAnimationLoop);\n      }\n    },\n    /**\n     * Helper function to clamp a value between min and max\n     */\n    clamp: function (value, min, max) {\n      return Math.min(Math.max(value, min), max);\n    },\n    /**\n     * Set up event listeners for audio data\n     */\n    setupEventListeners: function () {\n      // Listen for audio data\n      if (this.data.audioProcessor) {\n        this.data.audioProcessor.addEventListener('audiodata-updated', this.boundUpdateVisualization);\n      } else {\n        console.warn('wave3: No audio processor specified');\n      }\n\n      // Also listen for direct window messages (for back compatibility)\n      this.messageHandler = event => {\n        if (event.data && event.data.type === 'timeDomainData') {\n          this.updateVisualization({\n            detail: {\n              timeDomainData: event.data.data\n            }\n          });\n        }\n      };\n      window.addEventListener('message', this.messageHandler);\n    },\n    /**\n     * Force immediate render with dummy data for initial visibility\n     */\n    forceRender: function () {\n      const initialData = new Uint8Array(128);\n      for (let i = 0; i < initialData.length; i++) {\n        initialData[i] = 128 + Math.sin(i / 10) * 64;\n      }\n      this.updateVisualization({\n        detail: {\n          timeDomainData: initialData\n        }\n      });\n    },\n    /**\n     * Create the ring visualization using THREE.js lines\n     */\n    createRingVisualization: function () {\n      try {\n        const {\n          radius,\n          height,\n          segments,\n          lineWidth\n        } = this.data;\n\n        // Create a group to hold all our lines\n        const group = new THREE.Group();\n        this.lines = [];\n        this.lineVertices = [];\n\n        // Create lines around the ring\n        for (let i = 0; i < segments; i++) {\n          // Calculate angle for this segment\n          const angle = i / segments * Math.PI * 2;\n          const x = Math.cos(angle) * radius;\n          const z = Math.sin(angle) * radius;\n\n          // Create line geometry with points for the waveform\n          const points = [];\n          const verticalResolution = 20; // Number of points in each line\n\n          // Initial straight line\n          for (let j = 0; j < verticalResolution; j++) {\n            const y = j / (verticalResolution - 1) * height - height / 2;\n            points.push(new THREE.Vector3(0, y, 0));\n          }\n\n          // Create geometry from points\n          const geometry = new THREE.BufferGeometry().setFromPoints(points);\n\n          // Create gradient material\n          const color = this.getColorForPosition(i / segments);\n          const material = new THREE.LineBasicMaterial({\n            color: color,\n            linewidth: lineWidth,\n            transparent: true,\n            opacity: 0.8\n          });\n\n          // Create line mesh\n          const line = new THREE.Line(geometry, material);\n\n          // Position and rotate around the ring\n          line.position.set(x, 0, z);\n          line.lookAt(0, 0, 0);\n\n          // Store line and its vertices\n          group.add(line);\n          this.lines.push(line);\n          this.lineVertices.push(points);\n        }\n\n        // Set the group as the object3D\n        this.el.setObject3D('mesh', group);\n\n        // If successful, reset retry count\n        this.retryCount = 0;\n        console.log('Wave3 visualization created successfully');\n      } catch (error) {\n        console.error('Error creating wave3 visualization:', error);\n\n        // If we have retries left, try again after a delay\n        if (this.retryCount < this.maxRetries) {\n          this.retryCount++;\n          console.log(`Retrying wave3 visualization creation (${this.retryCount}/${this.maxRetries})...`);\n          setTimeout(() => this.createRingVisualization(), 500);\n        }\n      }\n    },\n    /**\n     * Get a color for a position in the ring (0-1)\n     */\n    getColorForPosition: function (position) {\n      // Convert position (0-1) to one of three color sections\n      if (position < 0.33) {\n        // Blend from start to middle\n        const t = position / 0.33;\n        return this.lerpColor(this.data.colorStart, this.data.colorMiddle, t);\n      } else if (position < 0.66) {\n        // Blend from middle to end\n        const t = (position - 0.33) / 0.33;\n        return this.lerpColor(this.data.colorMiddle, this.data.colorEnd, t);\n      } else {\n        // Blend from end back to start\n        const t = (position - 0.66) / 0.34;\n        return this.lerpColor(this.data.colorEnd, this.data.colorStart, t);\n      }\n    },\n    /**\n     * Linear interpolation between two colors\n     */\n    lerpColor: function (colorA, colorB, t) {\n      const colorAObj = new THREE.Color(colorA);\n      const colorBObj = new THREE.Color(colorB);\n      const r = colorAObj.r + (colorBObj.r - colorAObj.r) * t;\n      const g = colorAObj.g + (colorBObj.g - colorAObj.g) * t;\n      const b = colorAObj.b + (colorBObj.b - colorAObj.b) * t;\n      return new THREE.Color(r, g, b);\n    },\n    /**\n     * Update visualization when audio data is received\n     */\n    updateVisualization: function (event) {\n      if (!event.detail || !event.detail.timeDomainData || !event.detail.frequencyData || !this.lines || !this.lines.length) {\n        console.warn('wave3: Missing data for update');\n        return;\n      }\n      const timeDomainData = event.detail.timeDomainData;\n      const frequencyData = event.detail.frequencyData;\n      const lines = this.lines;\n      const segments = this.data.segments;\n      const height = this.data.height;\n\n      // Analyze both time domain and frequency data\n      this.analyzeAudioData(timeDomainData, frequencyData);\n\n      // Apply smoothing if we have previous data\n      let timeDataToUse = this.smoothData(timeDomainData);\n      this.prevData = timeDataToUse.slice(0);\n\n      // Update each line's position and properties\n      for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n        const points = line.geometry.attributes.position;\n        const lineSegments = points.count;\n        for (let j = 0; j < lineSegments; j++) {\n          // Get base amplitude from time domain data\n          const dataIndex = Math.floor(j / lineSegments * timeDataToUse.length);\n          let amplitude = timeDataToUse[dataIndex] / 128.0 - 1.0;\n\n          // Modify amplitude based on frequency analysis\n          amplitude = this.modifyAmplitudeWithFrequency(amplitude, i, j, lineSegments);\n\n          // Calculate position with modified amplitude\n          const angle = j / lineSegments * Math.PI * 2;\n          const radiusOffset = this.calculateRadiusOffset(i, j, amplitude);\n          points.array[j * 3] = Math.cos(angle) * (this.data.radius + radiusOffset);\n          points.array[j * 3 + 1] = amplitude * height;\n          points.array[j * 3 + 2] = Math.sin(angle) * (this.data.radius + radiusOffset);\n        }\n\n        // Update colors based on audio characteristics\n        this.updateLineColors(line, i);\n        points.needsUpdate = true;\n      }\n    },\n    analyzeAudioData: function (timeDomainData, frequencyData) {\n      // Calculate frequency band powers\n      const sampleRate = 44100;\n      const binSize = sampleRate / (frequencyData.length * 2);\n      Object.keys(this.frequencyBands).forEach(band => {\n        const {\n          min,\n          max\n        } = this.frequencyBands[band];\n        const minBin = Math.floor(min / binSize);\n        const maxBin = Math.ceil(max / binSize);\n        let sum = 0;\n        for (let i = minBin; i < maxBin && i < frequencyData.length; i++) {\n          const value = (frequencyData[i] + 100) / 70;\n          sum += value;\n        }\n        this.bandPowers[band] = this.bandPowers[band] * 0.8 + sum / (maxBin - minBin) * 0.2;\n      });\n    },\n    modifyAmplitudeWithFrequency: function (amplitude, lineIndex, pointIndex, totalPoints) {\n      const {\n        bass,\n        midrange,\n        treble\n      } = this.bandPowers;\n      const frequencyInfluence = this.data.frequencyInfluence;\n      const bassInfluence = bass * Math.sin(pointIndex / totalPoints * Math.PI * 2);\n      const midInfluence = midrange * Math.cos(pointIndex / totalPoints * Math.PI);\n      const trebleInfluence = treble * Math.sin(pointIndex / totalPoints * Math.PI * 4);\n      return amplitude * (1 + frequencyInfluence * (bassInfluence + midInfluence + trebleInfluence));\n    },\n    calculateRadiusOffset: function (lineIndex, pointIndex, amplitude) {\n      const {\n        bass,\n        treble\n      } = this.bandPowers;\n      const bassOffset = bass * Math.sin(pointIndex / this.data.segments * Math.PI * 2) * 2;\n      const trebleOffset = treble * Math.cos(pointIndex / this.data.segments * Math.PI * 4) * 1;\n      return amplitude * 2 + (bassOffset + trebleOffset) * this.data.frequencyInfluence;\n    },\n    updateLineColors: function (line, lineIndex) {\n      const material = line.material;\n      const {\n        bass,\n        midrange,\n        treble\n      } = this.bandPowers;\n      const startColor = new THREE.Color(this.data.colorStart);\n      const middleColor = new THREE.Color(this.data.colorMiddle);\n      const endColor = new THREE.Color(this.data.colorEnd);\n      const color = new THREE.Color();\n      const bassWeight = Math.min(1, bass * 1.5);\n      const trebleWeight = Math.min(1, treble * 1.5);\n      color.r = startColor.r * bassWeight + middleColor.r * (1 - bassWeight - trebleWeight) + endColor.r * trebleWeight;\n      color.g = startColor.g * bassWeight + middleColor.g * (1 - bassWeight - trebleWeight) + endColor.g * trebleWeight;\n      color.b = startColor.b * bassWeight + middleColor.b * (1 - bassWeight - trebleWeight) + endColor.b * trebleWeight;\n      material.color.copy(color);\n    },\n    smoothData: function (data) {\n      const smoothed = new Uint8Array(data.length);\n      const responsivenessFactor = this.data.responsiveness;\n      this.smoothingFactor = 0.3 * responsivenessFactor;\n      if (this.prevData && this.prevData.length === data.length) {\n        for (let i = 0; i < data.length; i++) {\n          smoothed[i] = this.prevData[i] * (1 - this.smoothingFactor) + data[i] * this.smoothingFactor;\n        }\n        return smoothed;\n      }\n      return data;\n    },\n    /**\n     * Animation loop for dynamic effects\n     */\n    animationLoop: function (timestamp) {\n      // Update animation time\n      this.animationTime = timestamp || 0;\n      this.breathingPhase += this.data.breathingSpeed * 0.01;\n      this.colorPhase += 0.005;\n\n      // Apply breathing effect\n      if (this.data.breathingEffect && this.group) {\n        const breathingFactor = 1.0 + Math.sin(this.breathingPhase) * this.data.breathingIntensity;\n        this.group.scale.set(breathingFactor, breathingFactor, breathingFactor);\n      }\n\n      // Apply rotation effect\n      if (this.data.rotation && this.group) {\n        this.group.rotation.y += this.data.rotationSpeed * 0.01;\n      }\n\n      // Continue animation loop\n      this.animationFrame = requestAnimationFrame(this.boundAnimationLoop);\n    },\n    /**\n     * Cleanup on component removal\n     */\n    remove: function () {\n      // Cancel animation frame if it exists\n      if (this.animationFrame) {\n        cancelAnimationFrame(this.animationFrame);\n        this.animationFrame = null;\n      }\n\n      // Remove event listeners\n      if (this.data.audioProcessor) {\n        this.data.audioProcessor.removeEventListener('audiodata-updated', this.boundUpdateVisualization);\n      }\n\n      // Remove window message listener\n      if (this.messageHandler) {\n        window.removeEventListener('message', this.messageHandler);\n        this.messageHandler = null;\n      }\n\n      // Dispose of THREE.js resources\n      if (this.lines) {\n        this.lines.forEach(line => {\n          if (line.geometry) line.geometry.dispose();\n          if (line.material) line.material.dispose();\n        });\n        this.lines = [];\n      }\n\n      // Remove the object3D\n      if (this.el.getObject3D('mesh')) {\n        this.el.removeObject3D('mesh');\n      }\n    }\n  });\n  return true;\n}\n;\n\n//# sourceURL=webpack://reverb-xr/./src/scripts/visualizers/wave3-script.js?");

/***/ }),

/***/ "./src/stages/visualizer.js":
/*!**********************************!*\
  !*** ./src/stages/visualizer.js ***!
  \**********************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _scripts_visualizers_index_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../scripts/visualizers/index.js */ \"./src/scripts/visualizers/index.js\");\n/* harmony import */ var _scripts_audio_GPUAudioProcessor_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../scripts/audio/GPUAudioProcessor.js */ \"./src/scripts/audio/GPUAudioProcessor.js\");\n/* harmony import */ var _scripts_audio_AudioProcessor_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../scripts/audio/AudioProcessor.js */ \"./src/scripts/audio/AudioProcessor.js\");\n// Import all visualizer components and processors\n\n\n\n\n// Initialize when document is ready\ndocument.addEventListener('DOMContentLoaded', () => {\n  try {\n    // Register all visualizers\n    console.log('Registering visualizer components...');\n    (0,_scripts_visualizers_index_js__WEBPACK_IMPORTED_MODULE_0__.registerVisualizers)();\n\n    // Initialize GPU Audio Processor\n    window.gpuAudioProcessor = new _scripts_audio_GPUAudioProcessor_js__WEBPACK_IMPORTED_MODULE_1__.GPUAudioProcessor({\n      fftSize: 2048,\n      smoothingTimeConstant: 0.8\n    });\n\n    // Set up message handling\n    window.addEventListener('message', handleMessage);\n    window.parent.postMessage({\n      type: 'visualizer-ready'\n    }, '*');\n  } catch (error) {\n    console.error('Error initializing visualizers:', error);\n  }\n});\n\n// Message handling functions\nfunction handleMessage(event) {\n  if (event.source !== window.parent) return;\n  const message = event.data;\n  switch (message.type) {\n    case 'frequencyData':\n    case 'timeDomainData':\n      if (window.gpuAudioProcessor) {\n        updateProcessorData(message.type, message.data);\n      }\n      break;\n    case 'toggle':\n      toggleElement(message.element, message.visible);\n      break;\n  }\n}\nfunction updateProcessorData(type, data) {\n  if (!window.gpuAudioProcessor) return;\n  try {\n    if (type === 'frequencyData') {\n      const floatData = new Float32Array(data.length);\n      for (let i = 0; i < data.length; i++) {\n        floatData[i] = data[i] / 255.0 * 100.0 - 100.0;\n      }\n      window.gpuAudioProcessor.getFrequencyData().set(floatData);\n    } else if (type === 'timeDomainData') {\n      const floatData = new Float32Array(data.length);\n      for (let i = 0; i < data.length; i++) {\n        floatData[i] = data[i] / 128.0 - 1.0;\n      }\n      window.gpuAudioProcessor.getTimeDomainData().set(floatData);\n    }\n    window.gpuAudioProcessor.updateTextureData();\n\n    // Update audio-processor entity\n    const processorEntity = document.getElementById('audio-processor');\n    if (processorEntity) {\n      processorEntity.emit('audiodata-updated', {\n        frequencyData: data,\n        timeDomainData: type === 'timeDomainData' ? data : window.gpuAudioProcessor.getTimeDomainDataForAPI()\n      });\n    }\n  } catch (error) {\n    console.error('Error updating processor data:', error);\n  }\n}\nfunction toggleElement(elementType, visible) {\n  const elements = {\n    wave3: document.querySelector('[wave3-visualizer]'),\n    waveform: document.querySelector('[circular-waveform]'),\n    bars: document.querySelector('[frequency-bars]'),\n    skybox: document.querySelector('a-sky'),\n    lights: Array.from(document.querySelectorAll('a-light')),\n    ground: document.querySelector('#ground-plane')\n  };\n  const element = elements[elementType];\n  if (!element) return;\n  if (Array.isArray(element)) {\n    element.forEach(el => el.setAttribute('visible', visible));\n  } else {\n    element.setAttribute('visible', visible);\n  }\n}\n\n//# sourceURL=webpack://reverb-xr/./src/stages/visualizer.js?");

/***/ })

/******/ });
/************************************************************************/
/******/ // The module cache
/******/ var __webpack_module_cache__ = {};
/******/ 
/******/ // The require function
/******/ function __webpack_require__(moduleId) {
/******/ 	// Check if module is in cache
/******/ 	var cachedModule = __webpack_module_cache__[moduleId];
/******/ 	if (cachedModule !== undefined) {
/******/ 		return cachedModule.exports;
/******/ 	}
/******/ 	// Create a new module (and put it into the cache)
/******/ 	var module = __webpack_module_cache__[moduleId] = {
/******/ 		// no module.id needed
/******/ 		// no module.loaded needed
/******/ 		exports: {}
/******/ 	};
/******/ 
/******/ 	// Execute the module function
/******/ 	__webpack_modules__[moduleId](module, module.exports, __webpack_require__);
/******/ 
/******/ 	// Return the exports of the module
/******/ 	return module.exports;
/******/ }
/******/ 
/******/ // expose the modules object (__webpack_modules__)
/******/ __webpack_require__.m = __webpack_modules__;
/******/ 
/************************************************************************/
/******/ /* webpack/runtime/define property getters */
/******/ (() => {
/******/ 	// define getter functions for harmony exports
/******/ 	__webpack_require__.d = (exports, definition) => {
/******/ 		for(var key in definition) {
/******/ 			if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 				Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 			}
/******/ 		}
/******/ 	};
/******/ })();
/******/ 
/******/ /* webpack/runtime/ensure chunk */
/******/ (() => {
/******/ 	__webpack_require__.f = {};
/******/ 	// This file contains only the entry chunk.
/******/ 	// The chunk loading function for additional chunks
/******/ 	__webpack_require__.e = (chunkId) => {
/******/ 		return Promise.all(Object.keys(__webpack_require__.f).reduce((promises, key) => {
/******/ 			__webpack_require__.f[key](chunkId, promises);
/******/ 			return promises;
/******/ 		}, []));
/******/ 	};
/******/ })();
/******/ 
/******/ /* webpack/runtime/get javascript chunk filename */
/******/ (() => {
/******/ 	// This function allow to reference async chunks
/******/ 	__webpack_require__.u = (chunkId) => {
/******/ 		// return url for filenames based on template
/******/ 		return "" + chunkId + ".bundle.js";
/******/ 	};
/******/ })();
/******/ 
/******/ /* webpack/runtime/hasOwnProperty shorthand */
/******/ (() => {
/******/ 	__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ })();
/******/ 
/******/ /* webpack/runtime/make namespace object */
/******/ (() => {
/******/ 	// define __esModule on exports
/******/ 	__webpack_require__.r = (exports) => {
/******/ 		if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 			Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 		}
/******/ 		Object.defineProperty(exports, '__esModule', { value: true });
/******/ 	};
/******/ })();
/******/ 
/******/ /* webpack/runtime/import chunk loading */
/******/ (() => {
/******/ 	// no baseURI
/******/ 	
/******/ 	// object to store loaded and loading chunks
/******/ 	// undefined = chunk not loaded, null = chunk preloaded/prefetched
/******/ 	// [resolve, Promise] = chunk loading, 0 = chunk loaded
/******/ 	var installedChunks = {
/******/ 		"visualizer": 0
/******/ 	};
/******/ 	
/******/ 	var installChunk = (data) => {
/******/ 		var {__webpack_ids__, __webpack_modules__, __webpack_runtime__} = data;
/******/ 		// add "modules" to the modules object,
/******/ 		// then flag all "ids" as loaded and fire callback
/******/ 		var moduleId, chunkId, i = 0;
/******/ 		for(moduleId in __webpack_modules__) {
/******/ 			if(__webpack_require__.o(__webpack_modules__, moduleId)) {
/******/ 				__webpack_require__.m[moduleId] = __webpack_modules__[moduleId];
/******/ 			}
/******/ 		}
/******/ 		if(__webpack_runtime__) __webpack_runtime__(__webpack_require__);
/******/ 		for(;i < __webpack_ids__.length; i++) {
/******/ 			chunkId = __webpack_ids__[i];
/******/ 			if(__webpack_require__.o(installedChunks, chunkId) && installedChunks[chunkId]) {
/******/ 				installedChunks[chunkId][0]();
/******/ 			}
/******/ 			installedChunks[__webpack_ids__[i]] = 0;
/******/ 		}
/******/ 	
/******/ 	}
/******/ 	
/******/ 	__webpack_require__.f.j = (chunkId, promises) => {
/******/ 			// import() chunk loading for javascript
/******/ 			var installedChunkData = __webpack_require__.o(installedChunks, chunkId) ? installedChunks[chunkId] : undefined;
/******/ 			if(installedChunkData !== 0) { // 0 means "already installed".
/******/ 	
/******/ 				// a Promise means "currently loading".
/******/ 				if(installedChunkData) {
/******/ 					promises.push(installedChunkData[1]);
/******/ 				} else {
/******/ 					if(true) { // all chunks have JS
/******/ 						// setup Promise in chunk cache
/******/ 						var promise = import("./" + __webpack_require__.u(chunkId)).then(installChunk, (e) => {
/******/ 							if(installedChunks[chunkId] !== 0) installedChunks[chunkId] = undefined;
/******/ 							throw e;
/******/ 						});
/******/ 						var promise = Promise.race([promise, new Promise((resolve) => (installedChunkData = installedChunks[chunkId] = [resolve]))])
/******/ 						promises.push(installedChunkData[1] = promise);
/******/ 					}
/******/ 				}
/******/ 			}
/******/ 	};
/******/ 	
/******/ 	// no prefetching
/******/ 	
/******/ 	// no preloaded
/******/ 	
/******/ 	// no external install chunk
/******/ 	
/******/ 	// no on chunks loaded
/******/ })();
/******/ 
/************************************************************************/
/******/ 
/******/ // startup
/******/ // Load entry module and return exports
/******/ // This entry module can't be inlined because the eval devtool is used.
/******/ var __webpack_exports__ = __webpack_require__("./src/stages/visualizer.js");
/******/ 
